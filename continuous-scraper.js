#!/usr/bin/env node

/**
 * Continuous Scraper for Hot Money Honey
 * 
 * Runs continuously to discover:
 * 1. Startups from RSS feeds (TechCrunch, VentureBeat, etc.)
 * 2. VC firms and investors
 * 3. News articles and funding announcements
 * 
 * Usage:
 *   node continuous-scraper.js
 * 
 * Or run in background:
 *   nohup node continuous-scraper.js > scraper.log 2>&1 &
 */

const { execSync } = require('child_process');

// Configuration
const SCRAPE_INTERVAL = 30 * 60 * 1000; // 30 minutes
const STARTUP_DISCOVERY_INTERVAL = 60 * 60 * 1000; // 1 hour
const VC_ENRICHMENT_INTERVAL = 2 * 60 * 60 * 1000; // 2 hours

let lastStartupScrape = 0;
let lastVcEnrichment = 0;

console.log('ğŸš€ Starting Continuous Scraper for Hot Money Honey');
console.log('ğŸ“…', new Date().toISOString());
console.log('â±ï¸  RSS Scraping every 30 minutes');
console.log('ğŸ” Startup Discovery every 1 hour');
console.log('ğŸ’¼ VC Enrichment every 2 hours');
console.log('â”€'.repeat(60));

/**
 * Run modern startup discovery (replaces broken RSS)
 */
async function scrapeRSSFeeds() {
  console.log('\nğŸ“¡ [STARTUP DISCOVERY] Scraping startup databases...');
  console.log('â°', new Date().toISOString());
  console.log('â„¹ï¸  Replaced broken RSS feeds with direct database scraping');
  
  try {
    const output = execSync('node modern-startup-discovery.js', {
      cwd: __dirname,
      encoding: 'utf-8',
      timeout: 15 * 60 * 1000, // 15 minute timeout
      maxBuffer: 10 * 1024 * 1024 // 10MB buffer for large output
    });
    
    console.log(output);
    console.log('âœ… [RSS] Feed scrape completed successfully');
    return true;
  } catch (error) {
    console.error('âŒ [RSS] Feed scrape failed:', error.message);
    console.log('â„¹ï¸  This is normal if scraping takes a long time');
    console.log('ğŸ’¡ Check discovered_startups table for results');
    return false; // Don't crash, just log the error
  }
}

/**
 * Discover startups from RSS articles
 */
async function discoverStartups() {
  const now = Date.now();
  if (now - lastStartupScrape < STARTUP_DISCOVERY_INTERVAL) {
    return; // Skip if run recently
  }
  
  console.log('\nğŸ” [STARTUPS] Startup discovery runs with RSS scrape');
  console.log('âœ… [STARTUPS] Check discovered_startups table');
  lastStartupScrape = now;
  return true;
}

/**
 * Enrich VC firms with latest data
 */
async function enrichVCs() {
  const now = Date.now();
  if (now - lastVcEnrichment < VC_ENRICHMENT_INTERVAL) {
    return; // Skip if run recently
  }
  
  console.log('\nğŸ’¼ [VCs] Starting VC enrichment...');
  console.log('â°', new Date().toISOString());
  
  try {
    // Use raw SQL script to bypass PostgREST schema cache issues
    const fs = require('fs');
    if (!fs.existsSync('./enrich-raw-sql.js')) {
      console.log('â„¹ï¸  [VCs] VC enrichment script not found, skipping');
      return false;
    }
    
    const output = execSync('node enrich-raw-sql.js', {
      cwd: __dirname,
      encoding: 'utf-8',
      timeout: 15 * 60 * 1000 // 15 minute timeout
    });
    
    console.log(output);
    console.log('âœ… [VCs] Enrichment completed successfully');
    lastVcEnrichment = now;
    return true;
  } catch (error) {
    console.error('âŒ [VCs] Enrichment failed:', error.message);
    return false;
  }
}

/**
 * Main scraping loop
 */
async function runScrapingCycle() {
  console.log('\n' + '='.repeat(60));
  console.log('ğŸ”„ Starting new scraping cycle');
  console.log('='.repeat(60));
  
  // Always scrape RSS feeds
  await scrapeRSSFeeds();
  
  // Periodically discover startups
  await discoverStartups();
  
  // Periodically enrich VCs
  await enrichVCs();
  
  console.log('\nâœ… Scraping cycle complete');
  console.log('â³ Next cycle in 30 minutes');
  console.log('â”€'.repeat(60));
}

/**
 * Handle graceful shutdown
 */
process.on('SIGINT', () => {
  console.log('\n\nğŸ›‘ Shutting down continuous scraper...');
  console.log('ğŸ“…', new Date().toISOString());
  process.exit(0);
});

process.on('SIGTERM', () => {
  console.log('\n\nğŸ›‘ Shutting down continuous scraper...');
  console.log('ğŸ“…', new Date().toISOString());
  process.exit(0);
});

/**
 * Start the continuous scraper
 */
async function start() {
  // Run immediately on start
  await runScrapingCycle();
  
  // Then run every 30 minutes
  setInterval(async () => {
    await runScrapingCycle();
  }, SCRAPE_INTERVAL);
}

// Start the scraper
start().catch(error => {
  console.error('ğŸ’¥ Fatal error:', error);
  process.exit(1);
});
