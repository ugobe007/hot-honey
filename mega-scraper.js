#!/usr/bin/env node
/**
 * MEGA SCRAPER - Maximum Volume Discovery
 * 
 * Scrapes ALL sources from mega-source-list.txt
 * Goal: Find 100+ startups and investors per run
 * 
 * Run: node mega-scraper.js
 * PM2: pm2 start mega-scraper.js --cron-restart="0 0,6,12,18 * * *"
 */

require('dotenv').config();
const fs = require('fs');
const { execSync } = require('child_process');
const { createClient } = require('@supabase/supabase-js');

const SUPABASE_URL = process.env.VITE_SUPABASE_URL;
const SUPABASE_SERVICE_KEY = process.env.SUPABASE_SERVICE_KEY;

if (!SUPABASE_URL || !SUPABASE_SERVICE_KEY) {
  console.error('âŒ Missing Supabase credentials');
  process.exit(1);
}

const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_KEY);

// Load sources from file
function loadSources() {
  const sourceFile = './mega-source-list.txt';
  if (!fs.existsSync(sourceFile)) {
    console.error('âŒ mega-source-list.txt not found');
    process.exit(1);
  }
  
  return fs.readFileSync(sourceFile, 'utf-8')
    .split('\n')
    .map(line => line.trim())
    .filter(line => line && line.startsWith('http'));
}

async function scrapeUrl(url, index, total) {
  const shortUrl = url.replace(/^https?:\/\//, '').substring(0, 40);
  process.stdout.write(`[${index}/${total}] ${shortUrl}... `);
  
  try {
    // Scrape for both startups and investors
    const output = execSync(`node intelligent-scraper.js "${url}" auto`, {
      cwd: __dirname,
      encoding: 'utf-8',
      timeout: 120000, // 2 minute timeout
      stdio: 'pipe'
    });
    
    const startupMatch = output.match(/ğŸš€ Startups: (\d+) added/);
    const investorMatch = output.match(/ğŸ’¼ Investors: (\d+) added/);
    
    const startups = startupMatch ? parseInt(startupMatch[1]) : 0;
    const investors = investorMatch ? parseInt(investorMatch[1]) : 0;
    
    if (startups > 0 || investors > 0) {
      console.log(`âœ… +${startups} startups, +${investors} investors`);
    } else {
      console.log(`â­ï¸`);
    }
    
    return { startups, investors, success: true };
    
  } catch (error) {
    console.log(`âš ï¸`);
    return { startups: 0, investors: 0, success: false };
  }
}

async function getStats() {
  const { count: startups } = await supabase
    .from('startup_uploads')
    .select('*', { count: 'exact', head: true })
    .eq('status', 'approved');
  
  const { count: discovered } = await supabase
    .from('discovered_startups')
    .select('*', { count: 'exact', head: true });
  
  const { count: investors } = await supabase
    .from('investors')
    .select('*', { count: 'exact', head: true });
  
  return { startups, discovered, investors };
}

async function main() {
  const startTime = Date.now();
  
  console.log('\n');
  console.log('ğŸ”¥'.repeat(35));
  console.log('ğŸ”¥                                                                 ğŸ”¥');
  console.log('ğŸ”¥   MEGA SCRAPER - Maximum Volume Discovery                       ğŸ”¥');
  console.log('ğŸ”¥   Goal: 100+ new startups & investors per run                   ğŸ”¥');
  console.log('ğŸ”¥                                                                 ğŸ”¥');
  console.log('ğŸ”¥'.repeat(35));
  console.log(`\nâ° Started: ${new Date().toISOString()}`);
  
  // Get starting stats
  const beforeStats = await getStats();
  console.log(`\nğŸ“Š BEFORE: ${beforeStats.discovered} discovered, ${beforeStats.startups} approved, ${beforeStats.investors} investors`);
  
  // Load all sources
  const sources = loadSources();
  console.log(`\nğŸ“‹ Loaded ${sources.length} sources to scrape\n`);
  console.log('â•'.repeat(70));
  
  let totalStartups = 0;
  let totalInvestors = 0;
  let successCount = 0;
  
  for (let i = 0; i < sources.length; i++) {
    const result = await scrapeUrl(sources[i], i + 1, sources.length);
    totalStartups += result.startups;
    totalInvestors += result.investors;
    if (result.success) successCount++;
    
    // Rate limit: 2 seconds between requests
    if (i < sources.length - 1) {
      await new Promise(r => setTimeout(r, 2000));
    }
  }
  
  // Get ending stats
  const afterStats = await getStats();
  const duration = Math.round((Date.now() - startTime) / 1000 / 60);
  
  // Final report
  console.log('\n' + 'â•'.repeat(70));
  console.log('ğŸ“Š MEGA SCRAPER RESULTS');
  console.log('â•'.repeat(70));
  console.log(`\nâ° Completed: ${new Date().toISOString()}`);
  console.log(`â±ï¸  Duration: ${duration} minutes`);
  
  console.log(`\nğŸ“ˆ STARTUPS:`);
  console.log(`   â€¢ New discovered: ${afterStats.discovered - beforeStats.discovered}`);
  console.log(`   â€¢ Total discovered: ${afterStats.discovered}`);
  
  console.log(`\nğŸ’¼ INVESTORS:`);
  console.log(`   â€¢ New added: ${afterStats.investors - beforeStats.investors}`);
  console.log(`   â€¢ Total investors: ${afterStats.investors}`);
  
  console.log(`\nğŸ“Š SCRAPING STATS:`);
  console.log(`   â€¢ Sources scraped: ${sources.length}`);
  console.log(`   â€¢ Successful: ${successCount}`);
  console.log(`   â€¢ Success rate: ${((successCount / sources.length) * 100).toFixed(1)}%`);
  
  const totalNew = (afterStats.discovered - beforeStats.discovered) + (afterStats.investors - beforeStats.investors);
  console.log('\n' + 'â•'.repeat(70));
  console.log(`âœ¨ TOTAL NEW ENTITIES: +${totalNew}`);
  console.log('â•'.repeat(70) + '\n');
  
  // Log to database
  try {
    await supabase.from('scraper_logs').insert({
      level: 'info',
      message: `Mega Scraper: +${afterStats.discovered - beforeStats.discovered} startups, +${afterStats.investors - beforeStats.investors} investors`,
      metadata: {
        startups_discovered: afterStats.discovered - beforeStats.discovered,
        investors_added: afterStats.investors - beforeStats.investors,
        sources_scraped: sources.length,
        success_rate: ((successCount / sources.length) * 100).toFixed(1) + '%',
        duration_minutes: duration
      }
    });
  } catch (e) {
    // Ignore logging errors
  }
}

main().catch(console.error);
