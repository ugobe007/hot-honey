/**
 * Puppeteer-based Portfolio Scraper - Backend Version
 * Uses headless Chrome to bypass anti-bot protections and scrape JavaScript-rendered sites
 */

import puppeteer, { Browser, Page } from 'puppeteer';
import { supabase } from '../config/supabase.js';
import { openai } from '../config/openai.js';

interface ScraperSource {
  id: string;
  name: string;
  url: string;
  type: 'vc_portfolio' | 'accelerator' | 'manual';
}

interface ScrapedCompany {
  name: string;
  url: string;
  description?: string;
  entity_type: 'startup' | 'vc_firm' | 'accelerator';
}

export class PuppeteerScraperService {
  private browser: Browser | null = null;

  /**
   * Initialize browser
   */
  private async initBrowser(): Promise<Browser> {
    if (this.browser) return this.browser;

    console.log('üöÄ Launching stealth Chrome browser...');
    
    const launchOptions: any = {
      headless: true,
      args: [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-blink-features=AutomationControlled', // Hide automation
        '--disable-features=IsolateOrigins,site-per-process',
        '--window-size=1920,1080',
        '--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        // Additional stealth args
        '--disable-web-security',
        '--disable-features=VizDisplayCompositor',
      ],
    };

    // Use custom Chrome path if provided (for some hosting providers)
    if (process.env.PUPPETEER_EXECUTABLE_PATH) {
      launchOptions.executablePath = process.env.PUPPETEER_EXECUTABLE_PATH;
    }

    this.browser = await puppeteer.launch(launchOptions);

    console.log('‚úÖ Stealth browser launched with anti-detection measures');
    return this.browser;
  }

  /**
   * Scrape portfolio page with Puppeteer
   */
  async scrapePortfolioPage(sourceId: string): Promise<any> {
    let page: Page | null = null;

    try {
      // Get source
      const { data: source, error: sourceError } = await supabase
        .from('scraper_sources')
        .select('*')
        .eq('id', sourceId)
        .single();

      if (sourceError || !source) {
        throw new Error(`Source not found: ${sourceError?.message}`);
      }

      // Create job
      const { data: job, error: jobError } = await supabase
        .from('scraper_jobs')
        .insert({
          source_id: sourceId,
          status: 'running',
          started_at: new Date().toISOString(),
          metadata: { source_name: source.name, source_url: source.url, method: 'puppeteer_backend' }
        })
        .select()
        .single();

      if (jobError || !job) {
        throw new Error(`Failed to create job: ${jobError?.message}`);
      }

      await this.log(job.id, 'info', `Starting Puppeteer scrape of ${source.name}`);

      // Initialize browser
      const browser = await this.initBrowser();
      page = await browser.newPage();

      // üé≠ ANTI-BOT DETECTION: Set realistic browser identity
      await page.setUserAgent(
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
      );

      // Set viewport with real device dimensions
      await page.setViewport({ width: 1920, height: 1080 });

      // üé≠ STEALTH: Override webdriver detection
      await page.evaluateOnNewDocument(() => {
        // Remove webdriver property
        Object.defineProperty(navigator, 'webdriver', {
          get: () => false,
        });
        
        // Override plugins to appear as real browser
        Object.defineProperty(navigator, 'plugins', {
          get: () => [1, 2, 3, 4, 5],
        });
        
        // Override languages
        Object.defineProperty(navigator, 'languages', {
          get: () => ['en-US', 'en'],
        });
        
        // Add chrome property (typical for real Chrome)
        (window as any).chrome = {
          runtime: {},
        };
        
        // Override permissions
        const originalQuery = window.navigator.permissions.query;
        window.navigator.permissions.query = (parameters: any) => (
          parameters.name === 'notifications' ?
            Promise.resolve({ state: Notification.permission } as PermissionStatus) :
            originalQuery(parameters)
        );
      });

      // üé≠ Set extra HTTP headers to appear human
      await page.setExtraHTTPHeaders({
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Referer': 'https://www.google.com/',
        'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'Sec-Ch-Ua-Mobile': '?0',
        'Sec-Ch-Ua-Platform': '"macOS"',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Upgrade-Insecure-Requests': '1',
      });

      console.log('üé≠ Stealth mode activated - browser identity configured');

      await this.log(job.id, 'info', `Navigating to ${source.url}...`);
      console.log(`üåê Navigating to ${source.url}...`);

      // Navigate to page
      await page.goto(source.url, {
        waitUntil: 'networkidle2',
        timeout: 60000
      });

      await this.log(job.id, 'info', 'Page loaded, waiting for content...');
      console.log('‚è≥ Waiting for content to load...');

      // üé≠ HUMAN-LIKE BEHAVIOR: Random scroll and mouse movements
      await page.evaluate(() => {
        window.scrollTo(0, Math.floor(Math.random() * 500));
      });
      
      // Wait with random delay (3-7 seconds) to mimic human behavior
      const randomDelay = 3000 + Math.floor(Math.random() * 4000);
      await new Promise(resolve => setTimeout(resolve, randomDelay));
      
      // üîÑ DYNAMIC CONTENT: Try to click "Load More" or similar buttons
      console.log('üîç Checking for Load More buttons...');
      try {
        const clicked = await page.evaluate(() => {
          const buttons = Array.from(document.querySelectorAll('button, a'));
          const loadMoreButton = buttons.find(btn => {
            const text = btn.textContent?.toLowerCase() || '';
            return text.includes('load more') || 
                   text.includes('show more') || 
                   text.includes('view all') ||
                   text.includes('see all');
          });
          
          if (loadMoreButton && loadMoreButton instanceof HTMLElement) {
            loadMoreButton.click();
            return true;
          }
          return false;
        });
        
        if (clicked) {
          console.log('‚úÖ Clicked Load More button, waiting for content...');
          await new Promise(resolve => setTimeout(resolve, 3000));
        } else {
          console.log('‚ÑπÔ∏è No Load More button found');
        }
      } catch (e) {
        console.log('‚ÑπÔ∏è Could not click Load More button:', e);
      }
      
      // Scroll down like a human reading to trigger lazy loading
      await page.evaluate(() => {
        window.scrollTo(0, document.body.scrollHeight / 2);
      });
      await new Promise(resolve => setTimeout(resolve, 1000));
      
      // Scroll to bottom to load all lazy content
      await page.evaluate(() => {
        window.scrollTo(0, document.body.scrollHeight);
      });
      await new Promise(resolve => setTimeout(resolve, 2000));
      
      console.log('üé≠ Human-like page interaction completed');

      // Get page HTML
      const html = await page.content();
      console.log(`üìÑ Retrieved ${html.length} characters of HTML`);
      await this.log(job.id, 'info', `Retrieved ${html.length} characters of HTML`);

      // Extract text content (easier for AI to parse)
      const textContent = await page.evaluate(() => {
        // Remove scripts, styles, etc.
        const clone = document.body.cloneNode(true) as HTMLElement;
        clone.querySelectorAll('script, style, noscript').forEach(el => el.remove());
        return clone.innerText;
      });

      console.log(`üìù Extracted ${textContent.length} characters of text`);

      // Get all links
      const links = await page.evaluate(() => {
        return Array.from(document.querySelectorAll('a[href]'))
          .map(a => ({
            text: (a as HTMLAnchorElement).textContent?.trim() || '',
            href: (a as HTMLAnchorElement).href
          }))
          .filter(link => link.text && link.href);
      });

      console.log(`üîó Found ${links.length} total links`);
      await this.log(job.id, 'info', `Found ${links.length} links on page`);

      // Use AI to extract companies from combined data
      const companies = await this.extractCompaniesWithAI(html, textContent, links, source, job.id);

      // Save results
      await this.saveResults(job.id, sourceId, companies);

      // Update job
      const { data: updatedJob } = await supabase
        .from('scraper_jobs')
        .update({
          status: 'completed',
          completed_at: new Date().toISOString(),
          companies_found: companies.length
        })
        .eq('id', job.id)
        .select()
        .single();

      // Update source
      await supabase
        .from('scraper_sources')
        .update({
          last_scraped_at: new Date().toISOString(),
          company_count: companies.length
        })
        .eq('id', sourceId);

      await this.log(job.id, 'info', `‚úÖ Complete! Found ${companies.length} companies`);
      console.log(`‚úÖ Scraping complete! Found ${companies.length} companies`);

      // Close page
      await page.close();

      return updatedJob || job;

    } catch (error: any) {
      console.error('‚ùå Puppeteer scraper error:', error);
      
      // Try to update job status to failed
      try {
        await supabase
          .from('scraper_jobs')
          .update({
            status: 'failed',
            completed_at: new Date().toISOString(),
            error_message: error.message
          })
          .eq('id', sourceId);
      } catch (updateError) {
        console.error('Failed to update job status:', updateError);
      }
      
      throw error;
    } finally {
      if (page) {
        await page.close().catch(() => {});
      }
    }
  }

  /**
   * Use AI to extract companies from page data
   */
  private async extractCompaniesWithAI(
    html: string,
    textContent: string,
    links: Array<{text: string, href: string}>,
    source: ScraperSource,
    jobId: string
  ): Promise<ScrapedCompany[]> {
    try {
      await this.log(jobId, 'info', 'Analyzing with AI...');
      console.log('ü§ñ Sending data to AI for analysis...');

      // Prepare data for AI
      const linksSample = links.slice(0, 200); // First 200 links
      const textSample = textContent.slice(0, 10000); // First 10k chars of text

      const prompt = `You are analyzing a ${source.type === 'vc_portfolio' ? 'VC portfolio' : 'accelerator portfolio'} page from ${source.name}.

Extract ALL portfolio companies (startups they've invested in or accelerated).

DATA PROVIDED:
1. Page text excerpt: "${textSample}"
2. All links with anchor text (showing ${linksSample.length} links):
${linksSample.map((l, i) => `${i + 1}. "${l.text}" -> ${l.href}`).join('\n')}

TASK:
Identify which links are portfolio companies. Look for:
- Company names in link text
- Links to company websites (not internal pages, social media, or navigation)
- Startup/company mentions in the text

Return ONLY valid JSON array:
[
  {
    "name": "Company Name",
    "url": "https://company.com",
    "description": "Brief description if available",
    "entity_type": "startup"
  }
]

Extract ALL portfolio companies you can identify. Return [] if none found.`;

      const completion = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content: 'You are an expert at analyzing VC/accelerator portfolio pages. Extract all portfolio company information and return valid JSON. Be thorough.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.1,
        max_tokens: 4000
      });

      const responseText = completion.choices[0]?.message?.content || '[]';
      console.log('ü§ñ AI Response:', responseText.slice(0, 500) + '...');

      // Parse JSON
      let jsonText = responseText.trim();
      if (jsonText.startsWith('```json')) {
        jsonText = jsonText.replace(/```json\n?/g, '').replace(/```\n?/g, '');
      } else if (jsonText.startsWith('```')) {
        jsonText = jsonText.replace(/```\n?/g, '');
      }

      const companies = JSON.parse(jsonText) as ScrapedCompany[];
      
      console.log(`üìä AI extracted ${companies.length} companies`);
      
      // üîç DIAGNOSTIC: Log details when no companies found
      if (companies.length === 0) {
        console.log(`‚ö†Ô∏è AI RETURNED 0 COMPANIES - DIAGNOSTIC INFO:`);
        console.log(`  - HTML length received: ${html.length} characters`);
        console.log(`  - Text content length: ${textContent.length} characters`);
        console.log(`  - Links found on page: ${links.length}`);
        console.log(`  - Full AI response text:`, responseText);
        console.log(`  - Parsed JSON:`, JSON.stringify(companies, null, 2));
      }
      
      await this.log(jobId, 'info', `AI extracted ${companies.length} companies`);

      // Diagnostic logging when no companies found
      if (companies.length === 0) {
        console.log('‚ö†Ô∏è DIAGNOSTIC: AI returned 0 companies');
        console.log(`- HTML length: ${html.length} chars`);
        console.log(`- Text content length: ${textContent.length} chars`);
        console.log(`- Links found: ${links.length}`);
        console.log(`- Full AI response:`, responseText);
        await this.log(jobId, 'warning', `AI returned 0 companies. HTML: ${html.length} chars, Text: ${textContent.length} chars, Links: ${links.length}`);
      }

      // Validate URLs
      const validCompanies = companies.filter(c => {
        try {
          new URL(c.url);
          return c.name && c.url;
        } catch {
          return false;
        }
      });

      console.log(`‚úÖ ${validCompanies.length} companies have valid URLs`);

      return validCompanies;

    } catch (error: any) {
      console.error('‚ùå AI extraction error:', error);
      await this.log(jobId, 'error', `AI extraction failed: ${error.message}`);
      return [];
    }
  }

  /**
   * Save results to database
   */
  private async saveResults(jobId: string, sourceId: string, companies: ScrapedCompany[]): Promise<void> {
    try {
      const results = companies.map(company => ({
        job_id: jobId,
        source_id: sourceId,
        company_name: company.name,
        company_url: company.url,
        status: 'found',
        entity_type: company.entity_type,
        enriched_data: { description: company.description }
      }));

      if (results.length > 0) {
        const { error } = await supabase
          .from('scraper_results')
          .upsert(results, {
            onConflict: 'company_url,source_id',
            ignoreDuplicates: false
          });

        if (error) throw error;

        console.log(`üíæ Saved ${results.length} companies to database`);
        await this.log(jobId, 'info', `Saved ${results.length} companies`);
      }
    } catch (error: any) {
      console.error('‚ùå Save error:', error);
      throw error;
    }
  }

  /**
   * Log events
   */
  private async log(jobId: string, level: 'info' | 'warn' | 'error', message: string): Promise<void> {
    try {
      await supabase.from('scraper_logs').insert({
        job_id: jobId,
        level,
        message,
        metadata: {}
      });
    } catch (error) {
      console.error('Failed to write log:', error);
    }
  }

  /**
   * Close browser
   */
  async close(): Promise<void> {
    if (this.browser) {
      await this.browser.close();
      this.browser = null;
      console.log('üîí Browser closed');
    }
  }
}
