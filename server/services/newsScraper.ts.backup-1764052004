import puppeteer, { Browser, Page } from 'puppeteer';
import OpenAI from 'openai';
import { createClient } from '@supabase/supabase-js';

const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_KEY!
);

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

interface NewsSource {
  name: string;
  searchUrl: string; // URL template with {vc_name} placeholder
  type: 'news' | 'press_release';
}

// Pre-configured news sources for VC investment announcements
const NEWS_SOURCES: NewsSource[] = [
  {
    name: 'TechCrunch Search',
    searchUrl: 'https://techcrunch.com/?s={vc_name}+raises+million+led',
    type: 'news'
  },
  {
    name: 'Crunchbase News',
    searchUrl: 'https://news.crunchbase.com/?s={vc_name}+funding+round+million',
    type: 'news'
  },
  {
    name: 'VentureBeat Search',
    searchUrl: 'https://venturebeat.com/?s={vc_name}+raises+Series+led',
    type: 'news'
  },
  {
    name: 'Business Wire',
    searchUrl: 'https://www.businesswire.com/portal/site/home/search/?searchType=news&searchTerm={vc_name}+announces+investment&searchPage=1',
    type: 'press_release'
  },
  {
    name: 'PR Newswire',
    searchUrl: 'https://www.prnewswire.com/search/news/?keyword={vc_name}+Series+million',
    type: 'press_release'
  }
];

export class NewsScraper {
  private browser: Browser | null = null;

  async initBrowser() {
    console.log('üåê Initializing browser for news scraping...');
    this.browser = await puppeteer.launch({
      headless: true,
      args: [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-blink-features=AutomationControlled',
        '--disable-features=IsolateOrigins,site-per-process',
        '--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        '--disable-web-security',
        '--disable-features=VizDisplayCompositor',
      ],
    });
    console.log('‚úÖ Browser initialized');
  }

  async closeBrowser() {
    if (this.browser) {
      await this.browser.close();
      this.browser = null;
      console.log('üîí Browser closed');
    }
  }

  /**
   * Scrape news articles for a specific VC firm's investments
   */
  async scrapeNewsForVC(vcName: string, jobId: string): Promise<any[]> {
    // Always create fresh browser for each scrape to avoid crashes
    await this.initBrowser();

    console.log(`\nüöÄ Starting NEWS scrape for: ${vcName}`);
    await this.log(jobId, 'info', `Starting news scrape for ${vcName}`);

    const allCompanies: any[] = [];
    let totalArticles = 0;
    let sourcesChecked = 0;

    try {

    // Try each news source
    for (const source of NEWS_SOURCES) {
      sourcesChecked++;
      await this.updateJobProgress(jobId, {
        sources_checked: sourcesChecked,
        articles_found: totalArticles,
        companies_found: allCompanies.length,
        current_source: source.name,
      });
      try {
        console.log(`\nüì∞ Checking ${source.name}...`);
        const url = source.searchUrl.replace('{vc_name}', encodeURIComponent(vcName));
        
        const result = await this.scrapeNewsSource(url, vcName, source.type, jobId);
        
        totalArticles += result.articlesFound;
        
        if (result.companies.length > 0) {
          console.log(`‚úÖ Found ${result.companies.length} companies from ${source.name}`);
          allCompanies.push(...result.companies);
        } else {
          console.log(`‚ÑπÔ∏è No results from ${source.name}`);
        }
        
        // Update progress after each source
        await this.updateJobProgress(jobId, {
          sources_checked: sourcesChecked,
          articles_found: totalArticles,
          companies_found: allCompanies.length,
          current_source: `Completed: ${source.name}`,
        });

        // Be respectful - wait between sources
        await new Promise(resolve => setTimeout(resolve, 2000));
        
      } catch (error) {
        console.error(`‚ùå Error scraping ${source.name}:`, error);
        await this.log(jobId, 'error', `Failed to scrape ${source.name}: ${error}`);
      }
    }

    // Deduplicate by company name
    const uniqueCompanies = this.deduplicateCompanies(allCompanies);
    console.log(`\nüìä Total unique companies found: ${uniqueCompanies.length}`);
    
    return uniqueCompanies;
    
    } finally {
      // Always close browser to prevent memory leaks
      await this.closeBrowser();
    }
  }

  /**
   * Scrape a single news source
   */
  private async scrapeNewsSource(
    url: string, 
    vcName: string, 
    sourceType: 'news' | 'press_release',
    jobId: string
  ): Promise<{ companies: any[], articlesFound: number }> {
    const page = await this.browser!.newPage();

    try {
      // Set viewport and user agent
      await page.setViewport({ width: 1920, height: 1080 });
      await page.setUserAgent(
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
      );

      // Navigate to the search results
      console.log(`üîó Loading: ${url}`);
      await page.goto(url, {
        waitUntil: 'networkidle2',
        timeout: 30000,
      });

      // Wait for content to load
      await new Promise(resolve => setTimeout(resolve, 3000));

      // Extract article headlines and snippets
      const articles = await page.evaluate(() => {
        const results: any[] = [];
        
        // Common article selectors for news sites
        const articleSelectors = [
          'article',
          '.post',
          '.article',
          '.news-item',
          '.search-result',
          '[class*="post-"]',
          '[class*="article-"]',
        ];

        const articles = document.querySelectorAll(articleSelectors.join(', '));
        
        articles.forEach((article, index) => {
          if (index > 20) return; // Limit to first 20 articles
          
          const titleEl = article.querySelector('h1, h2, h3, h4, .title, [class*="title"], [class*="headline"]');
          const linkEl = article.querySelector('a');
          const snippetEl = article.querySelector('p, .excerpt, .description, [class*="excerpt"], [class*="description"]');
          
          if (titleEl && linkEl) {
            results.push({
              title: titleEl.textContent?.trim() || '',
              url: linkEl.getAttribute('href') || '',
              snippet: snippetEl?.textContent?.trim() || '',
            });
          }
        });

        return results;
      });

      console.log(`üìÑ Found ${articles.length} articles`);

      // Use AI to extract company information from headlines
      if (articles.length > 0) {
        const companies = await this.extractCompaniesFromNews(articles, vcName, sourceType);
        await page.close();
        return { companies, articlesFound: articles.length };
      }

      await page.close();
      return { companies: [], articlesFound: 0 };

    } catch (error) {
      console.error(`‚ùå Error scraping news source:`, error);
      await page.close();
      return { companies: [], articlesFound: 0 };
    }
  }

  /**
   * Use AI to extract company information from news articles
   */
  private async extractCompaniesFromNews(
    articles: any[],
    vcName: string,
    sourceType: string
  ): Promise<any[]> {
    const articlesText = articles
      .map((a, i) => `${i + 1}. ${a.title}\n${a.snippet}\nURL: ${a.url}`)
      .join('\n\n');

    const prompt = `You are analyzing ${sourceType} articles about venture capital investments by ${vcName}.

Extract ONLY companies that received funding/investment from ${vcName} based on these article headlines and snippets:

${articlesText}

For each company that received investment from ${vcName}, return a JSON object with:
- name: Company name
- description: What the company does (from the article)
- funding_stage: Stage of funding (seed, Series A, etc.) if mentioned
- article_url: URL of the article
- investment_date: Date if mentioned (format: YYYY-MM-DD), otherwise null

Return ONLY valid JSON array. If no clear investments found, return empty array [].
Only include companies where ${vcName} is explicitly mentioned as an investor.

Format: [{"name":"CompanyName","description":"...","funding_stage":"Series A","article_url":"https://...","investment_date":"2024-01-15"}]`;

    try {
      console.log('ü§ñ Analyzing articles with AI...');
      
      const response = await openai.chat.completions.create({
        model: 'gpt-4',
        messages: [
          {
            role: 'system',
            content: 'You are a VC investment tracking assistant. Extract company information from funding news articles. Include any company mentioned in funding news where the VC firm is named as an investor or participant. Be inclusive - if the VC is mentioned in connection with a company\'s funding, include it.',
          },
          {
            role: 'user',
            content: prompt,
          },
        ],
        temperature: 0.1,
        max_tokens: 2000,
      });

      const content = response.choices[0].message.content || '';
      console.log('ü§ñ AI Response:', content.substring(0, 200) + '...');

      // Parse JSON from response
      const jsonMatch = content.match(/\[[\s\S]*\]/);
      if (jsonMatch) {
        const companies = JSON.parse(jsonMatch[0]);
        console.log(`üìä AI extracted ${companies.length} companies`);
        return companies;
      }

      console.log('‚ö†Ô∏è No valid JSON found in AI response');
      return [];

    } catch (error) {
      console.error('‚ùå Error extracting companies with AI:', error);
      return [];
    }
  }

  /**
   * Deduplicate companies by name
   */
  private deduplicateCompanies(companies: any[]): any[] {
    const seen = new Map<string, any>();
    
    for (const company of companies) {
      const normalizedName = company.name.toLowerCase().trim();
      if (!seen.has(normalizedName)) {
        seen.set(normalizedName, company);
      } else {
        // Merge information if we see the same company again
        const existing = seen.get(normalizedName)!;
        if (!existing.description && company.description) {
          existing.description = company.description;
        }
        if (!existing.funding_stage && company.funding_stage) {
          existing.funding_stage = company.funding_stage;
        }
      }
    }

    return Array.from(seen.values());
  }

  /**
   * Save companies to database
   */
  async saveCompaniesToDB(companies: any[], jobId: string, sourceId: string): Promise<void> {
    console.log(`\nüíæ Saving ${companies.length} companies to database...`);

    for (const company of companies) {
      try {
        const { error } = await supabase.from('scraper_results').insert({
          job_id: jobId,
          source_id: sourceId,
          company_name: company.name,
          description: company.description || null,
          website: null, // We don't have website from news
          funding_stage: company.funding_stage || null,
          source_url: company.article_url || null,
          raw_data: company,
        });

        if (error) {
          console.error(`‚ùå Error saving ${company.name}:`, error.message);
        }
      } catch (err) {
        console.error(`‚ùå Error saving company:`, err);
      }
    }

    console.log('‚úÖ Companies saved to database');
  }

  /**
   * Log message to database
   */
  private async log(jobId: string, level: string, message: string): Promise<void> {
    try {
      await supabase.from('scraper_logs').insert({
        job_id: jobId,
        level,
        message,
      });
    } catch (error) {
      // Silently fail if scraper_logs table doesn't exist
    }
  }

  /**
   * Update job status
   */
  async updateJobStatus(
    jobId: string,
    status: string,
    companiesFound?: number,
    error?: string
  ): Promise<void> {
    const updates: any = { status };
    
    if (companiesFound !== undefined) {
      updates.companies_found = companiesFound;
    }
    
    if (error) {
      updates.error_message = error;
    }
    
    if (status === 'completed' || status === 'failed') {
      updates.completed_at = new Date().toISOString();
    }

    await supabase.from('scraper_jobs').update(updates).eq('id', jobId);
  }

  /**
   * Update job progress with detailed stats
   */
  async updateJobProgress(
    jobId: string,
    stats: {
      sources_checked?: number;
      articles_found?: number;
      companies_found?: number;
      current_source?: string;
    }
  ): Promise<void> {
    const updates: any = {};
    
    if (stats.companies_found !== undefined) {
      updates.companies_found = stats.companies_found;
    }
    
    // Store progress in metadata JSON field
    const metadata = {
      sources_checked: stats.sources_checked || 0,
      articles_found: stats.articles_found || 0,
      current_source: stats.current_source || null,
      last_update: new Date().toISOString(),
    };
    
    updates.metadata = metadata;

    await supabase.from('scraper_jobs').update(updates).eq('id', jobId);
  }
}

// Export singleton instance
export const newsScraper = new NewsScraper();
